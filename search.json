[{"path":"https://simonpcouch.github.io/metalearners/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Posit Software, PBC Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://simonpcouch.github.io/metalearners/articles/analysis.html","id":"overview-of-stacking","dir":"Articles","previous_headings":"","what":"Overview of stacking","title":"Generalizing the meta-learner in stacks","text":"highest level, ensembles formed model definitions. package, model definitions instance minimal workflow, containing model specification (defined parsnip package) , optionally, preprocessor (defined recipes package). Model definitions specify form candidate ensemble members.  used ensemble, model definitions must share resample. rsample rset object, paired model definitions, can used generate tuning/fitting results objects candidate ensemble members tune.  Candidate members first come together data_stack object add_candidates() function. Principally, data stacks just tibbles, first column gives true outcome assessment set (portion training set used model validation), remaining columns give predictions candidate ensemble member. also bring along extra attributes keep track model definitions.  , data stack can evaluated using blend_predictions() determine best combine outputs candidate members using regularized linear regression model. stacking literature, process commonly called metalearning. outputs member likely highly correlated. Thus, depending degree regularization choose, coefficients inputs (possibly) many members zero —predictions influence final output, terms thus thrown .  stacking coefficients determine candidate ensemble members become ensemble members. Candidates non-zero stacking coefficients fitted whole training set, altogether making model_stack object.  model stack object, outputted fit_members(), ready predict new data! trained ensemble members often referred base models stacking literature.","code":""},{"path":"https://simonpcouch.github.io/metalearners/articles/analysis.html","id":"generalizing-the-meta-learner","dir":"Articles","previous_headings":"","what":"Generalizing the meta-learner","title":"Generalizing the meta-learner in stacks","text":"Since released package, question ’ve received far often whether ’ll introduce support stacking models meta-learner regularized linear regression model. , asked tidymodels users whether feature one ’d interested part 2024 tidymodels user survey, got resounding “yes.” good reasons initially implemented stacking regularized linear models: original paper proposing stacking regularized linear model. Regularized linear models elastic net handle correlated predictors quite well. definition, predictors supplied meta-learner highly correlated (least ), since predictors predictions outcome bunch different statistical models. Regularized linear models elastic net “zero ” predictors (.e. shrink associated coefficients zero). case stacking, zeroing predictors really advantageous, since models whose predictors associated nonzero coefficients—“members”—must trained full training set. Training members (fit_members()) one three highly computationally intensive steps model stacking, reducing number members need trained can greatly reduce amount time required create model stack. Simon tired student wrote first versions package. reasons (maybe omitting last), regularized linear models far popular meta-learner used literature. said, sorts methods handle correlated data disinclude certain predictors statistical models. considering whether implement functionality, decided conduct series experiments benchmark predictive performance time--fit variety meta-learners.","code":""},{"path":"https://simonpcouch.github.io/metalearners/articles/analysis.html","id":"the-experiments","dir":"Articles","previous_headings":"","what":"The experiments","title":"Generalizing the meta-learner in stacks","text":"decide whether implement generalized meta-learner stacks, conducted series experiments answer question: meta-learners reliably fit quickly provide performant predictions regularized linear model across contexts? first built rough proof--concept interface generalized meta-learner stacks. user interface definitely improvements make, gave us good-enough tool experiment . , collected set 8 machine learning tasks varied outcome type (numeric factor), dataset size, correlation structure. , tasks, : Interactively developed models best predict outcome, form candidate members. Fitted candidate members. Note breaks usual flow model stacking stacks; reduce duplicated member fits, decided first fit candidate members “drop place” needed. (language package, stacks() %>% add_candidates(...) %>% fit_members() %>% blend_predictions() rather usual stacks() %>% add_candidates(...) %>% blend_predictions() %>% fit_members().) Resampled set 12 proposed meta-learners hypothesized perform well, benchmarking timings . Generated predictions resampled meta-learners evaluated predictions metrics. Finally, collated metrics benchmarks, well related metadata, dataset metalearners, now available data package CRAN. 12 proposed meta-learners mentioned Step 3): exact definitions recipes model specifications can found experiment repository. resulting dataset containing experiment metadata, performance metrics, speed benchmarks available metalearners data package: row resulting dataset represents unique combination task, proposed meta-learner (recipe + specification), performance metric. meta-learner recipe == \"basic\" spec == \"glmnet\" existing, regularized linear model stacks.","code":"library(metalearners)  metalearners ## # A tibble: 217 × 8 ##    task   meta_learner   recipe  specification elapsed metric estimate estimator ##    <fct>  <fct>          <fct>   <fct>           <dbl> <chr>     <dbl> <chr>     ##  1 barley minimal_glmnet Minimal GLM (glmnet)    0.628 rsq       0.977 standard  ##  2 barley minimal_glmnet Minimal GLM (glmnet)    0.628 rmse      2.29  standard  ##  3 barley minimal_lgb    Minimal Boosted Tree… 194.    rsq       0.966 standard  ##  4 barley minimal_lgb    Minimal Boosted Tree… 194.    rmse      3.13  standard  ##  5 barley minimal_xgb    Minimal Boosted Tree… 263.    rsq       0.973 standard  ##  6 barley minimal_xgb    Minimal Boosted Tree… 263.    rmse      2.53  standard  ##  7 barley normalize_bt   Center… Bagged Tree …  25.7   rsq       0.970 standard  ##  8 barley normalize_bt   Center… Bagged Tree …  25.7   rmse      2.63  standard  ##  9 barley normalize_nn   Center… Neural Netwo… 407.    rsq       0.859 standard  ## 10 barley normalize_nn   Center… Neural Netwo… 407.    rmse      5.75  standard  ## # ℹ 207 more rows"},{"path":"https://simonpcouch.github.io/metalearners/articles/analysis.html","id":"results","dir":"Articles","previous_headings":"","what":"Results","title":"Generalizing the meta-learner in stacks","text":"Revisiting original research question, ’re interested distributions 1) elapsed time--fit benchmarks 2) performance metrics proposed meta-learner. first one bit easier analyze, ’ll start .","code":"library(tidyverse) library(gt) library(monochromeR)"},{"path":"https://simonpcouch.github.io/metalearners/articles/analysis.html","id":"elapsed-time","dir":"Articles","previous_headings":"Results","what":"Elapsed time","title":"Generalizing the meta-learner in stacks","text":"multiple performance metrics unique combination task meta-learner, one fit time. makes Perhaps best expressed table: fastest model fits shaded lightest reds, nearly belong glmnet. Maybe telling statistic: another meta-learner ever resample quickly glmnet? Almost batting 1000. conclusion definitely seems , data structure like data stack, regularized linear model like glmnet quickest-fitting among models tried. However, meta-learners ’ve proposed deliver consistently improved predictive performance, still consider implementing generalized meta-learner.","code":"metalearners_times <-    metalearners %>%    slice_head(n = 1, by = c(task, recipe, specification))  metalearners_times ## # A tibble: 87 × 8 ##    task   meta_learner   recipe  specification elapsed metric estimate estimator ##    <fct>  <fct>          <fct>   <fct>           <dbl> <chr>     <dbl> <chr>     ##  1 barley minimal_glmnet Minimal GLM (glmnet)    0.628 rsq       0.977 standard  ##  2 barley minimal_lgb    Minimal Boosted Tree… 194.    rsq       0.966 standard  ##  3 barley minimal_xgb    Minimal Boosted Tree… 263.    rsq       0.973 standard  ##  4 barley normalize_bt   Center… Bagged Tree …  25.7   rsq       0.970 standard  ##  5 barley normalize_nn   Center… Neural Netwo… 407.    rsq       0.859 standard  ##  6 barley normalize_svm  Center… Support Vect…   6.14  rsq      NA     standard  ##  7 barley pca_bt         Princi… Bagged Tree …  20.4   rsq       0.970 standard  ##  8 barley pca_nn         Princi… Neural Netwo… 199.    rsq       0.859 standard  ##  9 barley pca_svm        Princi… Support Vect…   6.05  rsq      NA     standard  ## 10 barley renormalize_nn C+S, P… Neural Netwo… 210.    rsq       0.865 standard  ## # ℹ 77 more rows metalearners_times %>%   summarize(     Minimum = min(elapsed),     `1st Quartile` = quantile(elapsed, .25),     Median = median(elapsed),     Mean = mean(elapsed),      `3rd Quartile` = quantile(elapsed, .75),     Maximum = max(elapsed),      .by = c(recipe, specification)   ) %>%   gt() %>%   fmt_number() %>%   cols_label_with(c(\"recipe\", \"specification\"), function(x) {md(paste0(\"`\", x, \"`\"))}) %>%   data_color(     columns = Minimum:Maximum,     method = \"quantile\",      palette = generate_palette(\"#bfa382\", \"go_lighter\", 20),     reverse = TRUE,     quantiles = 20   ) metalearners_times %>%   summarize(     glmnet_is_fastest =        all(elapsed[specification == \"GLM (glmnet)\"] <            elapsed[specification != \"GLM (glmnet)\"]),     .by = task   ) ## # A tibble: 8 × 2 ##   task   glmnet_is_fastest ##   <fct>  <lgl>             ## 1 barley TRUE              ## 2 caret  TRUE              ## 3 conc   TRUE              ## 4 hpc    FALSE             ## 5 kc     TRUE              ## 6 nhl    TRUE              ## 7 rare   TRUE              ## 8 wind   TRUE"},{"path":"https://simonpcouch.github.io/metalearners/articles/analysis.html","id":"performance-metrics","dir":"Articles","previous_headings":"Results","what":"Performance metrics","title":"Generalizing the meta-learner in stacks","text":"summarization results bit complicated comes performance metrics, multiple metrics (representing different interpretations predictive performance) experiment. Let’s start classification tasks, use common set metrics. First, transformation: , making use formatting magic gt package: table, darker green metric values indicate better average performance, darker red standard deviations indicate variation metric value task task. least far classification metrics go, ’s clear winner , though meta-learners making use support vector machines boosted trees seem well. regression, glmnet xgboost seem like winners; \\(R^2\\) RMSE values good gets, variation performance estimates across tasks varies least.","code":"metalearners_metrics <-   metalearners %>%   summarize(     mean = mean(estimate, na.rm = TRUE),     sd = sd(estimate, na.rm = TRUE),     .by = c(recipe, specification, metric)   ) %>%   pivot_wider(id_cols = c(recipe, specification), names_from = metric, values_from = c(mean, sd), names_vary = \"slowest\") gt(metalearners_metrics) %>%   fmt_number() %>%   tab_spanner(md(\"`rsq()`\"), contains(\"rsq\")) %>%   tab_spanner(md(\"`rmse()`\"), contains(\"rmse\")) %>%   tab_spanner(md(\"`accuracy()`\"), contains(\"accuracy\")) %>%   tab_spanner(md(\"`brier_class()`\"), contains(\"brier_class\")) %>%   tab_spanner(md(\"`roc_auc()`\"), contains(\"roc_auc\")) %>%   tab_spanner(     \"Regression\",      spanners = c(\"`rsq()`\", \"`rmse()`\")   ) %>%   tab_spanner(     \"Classification\",      spanners = c(\"`accuracy()`\", \"`brier_class()`\", \"`roc_auc()`\")   ) %>%   cols_label_with(fn = function(x) {     gsub(\"\\\\_accuracy|\\\\_brier\\\\_class|_roc\\\\_auc|\\\\_rsq|\\\\_rmse\", \"\", x)    }) %>%   # metrics where higher values mean better performance:   data_color(     c(\"mean_rsq\", \"mean_accuracy\", \"mean_roc_auc\"),      method = \"numeric\",      palette = generate_palette(\"#79b9aa\", \"go_lighter\", 10),     reverse = TRUE   ) %>%   # metrics where lower values mean better performance   data_color(     c(\"mean_rmse\", \"mean_brier_class\"),      method = \"numeric\",      palette = generate_palette(\"#79b9aa\", \"go_lighter\", 10),     reverse = FALSE   ) %>%   data_color(     contains(\"sd\"),      method = \"numeric\",      palette = generate_palette(\"#bfa382\", \"go_lighter\", 10),     reverse = TRUE   )"},{"path":"https://simonpcouch.github.io/metalearners/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Simon Couch. Author, maintainer. Max Kuhn. Author. Posit Software, PBC. Copyright holder, funder.","code":""},{"path":"https://simonpcouch.github.io/metalearners/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Couch S, Kuhn M (2024). metalearners: Experiments Model Stacking Alternatives. R package version 0.0.1, https://simonpcouch.github.io/metalearners/, https://github.com/simonpcouch/metalearners.","code":"@Manual{,   title = {metalearners: Experiments on Model Stacking Alternatives},   author = {Simon Couch and Max Kuhn},   year = {2024},   note = {R package version 0.0.1, https://simonpcouch.github.io/metalearners/},   url = {https://github.com/simonpcouch/metalearners}, }"},{"path":"https://simonpcouch.github.io/metalearners/index.html","id":"metalearners","dir":"","previous_headings":"","what":"Experiments on Model Stacking Alternatives","title":"Experiments on Model Stacking Alternatives","text":"goal metalearners provide dataset arose set experiments intended benchmark time--fit predictive performance number alternative model stacking methods (“meta-learners”) use stacks package.","code":""},{"path":"https://simonpcouch.github.io/metalearners/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Experiments on Model Stacking Alternatives","text":"can install development version metalearners like :","code":"pak::pak(\"simonpcouch/metalearners\")"},{"path":"https://simonpcouch.github.io/metalearners/index.html","id":"data","dir":"","previous_headings":"","what":"Data","title":"Experiments on Model Stacking Alternatives","text":"dataset metalearners package looks like :","code":"library(metalearners)  metalearners #> # A tibble: 217 × 8 #>    task   meta_learner   recipe  specification elapsed metric estimate estimator #>    <fct>  <fct>          <fct>   <fct>           <dbl> <chr>     <dbl> <chr>     #>  1 barley minimal_glmnet Minimal GLM (glmnet)    0.628 rsq       0.977 standard  #>  2 barley minimal_glmnet Minimal GLM (glmnet)    0.628 rmse      2.29  standard  #>  3 barley minimal_lgb    Minimal Boosted Tree… 194.    rsq       0.966 standard  #>  4 barley minimal_lgb    Minimal Boosted Tree… 194.    rmse      3.13  standard  #>  5 barley minimal_xgb    Minimal Boosted Tree… 263.    rsq       0.973 standard  #>  6 barley minimal_xgb    Minimal Boosted Tree… 263.    rmse      2.53  standard  #>  7 barley normalize_bt   Center… Bagged Tree …  25.7   rsq       0.970 standard  #>  8 barley normalize_bt   Center… Bagged Tree …  25.7   rmse      2.63  standard  #>  9 barley normalize_nn   Center… Neural Netwo… 407.    rsq       0.859 standard  #> 10 barley normalize_nn   Center… Neural Netwo… 407.    rmse      5.75  standard  #> # ℹ 207 more rows"},{"path":"https://simonpcouch.github.io/metalearners/reference/stacking-package.html","id":null,"dir":"Reference","previous_headings":"","what":"metalearners: Experiments on Model metalearners Alternatives — metalearners-package","title":"metalearners: Experiments on Model metalearners Alternatives — metalearners-package","text":"Supplies dataset arose set experiments intended benchmark time--fit predictive performance number alternative model stacking methods (\"meta-learners\") use stacks package.","code":""},{"path":"https://simonpcouch.github.io/metalearners/reference/stacking-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"metalearners: Experiments on Model metalearners Alternatives — metalearners-package","text":"Maintainer: Simon Couch simon.couch@posit.co Authors: Max Kuhn max@posit.co contributors: Posit Software, PBC [copyright holder, funder]","code":""},{"path":"https://simonpcouch.github.io/metalearners/reference/stacking.html","id":null,"dir":"Reference","previous_headings":"","what":"Data on model metalearners experiments — metalearners","title":"Data on model metalearners experiments — metalearners","text":"metalearners arose set experiments intended benchmark time--fit predictive performance number alternative model stacking methods (\"meta-learners\") use stacks package. row data represents unique combination task, proposed meta-learner (recipe + specification), performance metric. meta-learner minimal_glmnet existing meta-learner, remaining entries represent alternative meta-learners compared minimal_glmnet.","code":""},{"path":"https://simonpcouch.github.io/metalearners/reference/stacking.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data on model metalearners experiments — metalearners","text":"","code":"metalearners"},{"path":[]},{"path":"https://simonpcouch.github.io/metalearners/reference/stacking.html","id":"metalearners","dir":"Reference","previous_headings":"","what":"metalearners","title":"Data on model metalearners experiments — metalearners","text":"data frame 217 rows 8 columns: task Factor. descriptor modeling task, .e. dataset. meta_learner Factor. ID proposed meta-learner. proposed meta-learner made pairing recipe model specification. recipe Factor. recipe used preprocess data stack. See meta_learners folder linked repository implementation. specification Factor. model specification used model data stack. See meta_learners folder linked repository implementation. elapsed Numeric. elapsed time tune meta-learner resamples, seconds. \"real\" time rather CPU time. metric Character. type performance metric, returned yardstick package. estimate Character. performance estimate, returned yardstick package. estimator Character. performance estimator, returned yardstick package.","code":""},{"path":"https://simonpcouch.github.io/metalearners/reference/stacking.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Data on model metalearners experiments — metalearners","text":"https://github.com/simonpcouch/metalearner","code":""},{"path":"https://simonpcouch.github.io/metalearners/reference/stacking.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Data on model metalearners experiments — metalearners","text":"Every meta_learner evaluated every task. said, number meta-learners failed train given task, usually due memory exhaustion, failed experiments represented data. experiment carried 24-core machine 64GB memory running Linux Ubuntu 24.04. Computations distributed using forking 5 cores (except tasks caret nhl, used 2 cores.)","code":""},{"path":"https://simonpcouch.github.io/metalearners/reference/stacking.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data on model metalearners experiments — metalearners","text":"","code":"str(metalearners) #> tibble [217 × 8] (S3: tbl_df/tbl/data.frame) #>  $ task         : Factor w/ 8 levels \"barley\",\"caret\",..: 1 1 1 1 1 1 1 1 1 1 ... #>  $ meta_learner : Factor w/ 11 levels \"minimal_glmnet\",..: 1 1 2 2 3 3 4 4 5 5 ... #>  $ recipe       : Factor w/ 4 levels \"C+S, PCA, C+S\",..: 3 3 3 3 3 3 2 2 2 2 ... #>  $ specification: Factor w/ 6 levels \"Bagged Tree (rpart)\",..: 4 4 2 2 3 3 1 1 5 5 ... #>  $ elapsed      : num [1:217] 0.628 0.628 194.429 194.429 263.355 ... #>  $ metric       : chr [1:217] \"rsq\" \"rmse\" \"rsq\" \"rmse\" ... #>  $ estimate     : num [1:217] 0.977 2.288 0.966 3.134 0.973 ... #>  $ estimator    : chr [1:217] \"standard\" \"standard\" \"standard\" \"standard\" ...  metalearners #> # A tibble: 217 × 8 #>    task   meta_learner   recipe  specification elapsed metric estimate estimator #>    <fct>  <fct>          <fct>   <fct>           <dbl> <chr>     <dbl> <chr>     #>  1 barley minimal_glmnet Minimal GLM (glmnet)    0.628 rsq       0.977 standard  #>  2 barley minimal_glmnet Minimal GLM (glmnet)    0.628 rmse      2.29  standard  #>  3 barley minimal_lgb    Minimal Boosted Tree… 194.    rsq       0.966 standard  #>  4 barley minimal_lgb    Minimal Boosted Tree… 194.    rmse      3.13  standard  #>  5 barley minimal_xgb    Minimal Boosted Tree… 263.    rsq       0.973 standard  #>  6 barley minimal_xgb    Minimal Boosted Tree… 263.    rmse      2.53  standard  #>  7 barley normalize_bt   Center… Bagged Tree …  25.7   rsq       0.970 standard  #>  8 barley normalize_bt   Center… Bagged Tree …  25.7   rmse      2.63  standard  #>  9 barley normalize_nn   Center… Neural Netwo… 407.    rsq       0.859 standard  #> 10 barley normalize_nn   Center… Neural Netwo… 407.    rmse      5.75  standard  #> # ℹ 207 more rows"}]
